{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn, pennylane, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing kernel methods and variational quantum circuits on a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbltostibalduc/miniforge3/envs/plTutorialsEnv/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\n",
    "from pennylane.operation import Tensor\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a random number generator for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# pick only the first two classes (corresponding to the first 100 samples)\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "# scaling the data for better training and avoiding problems due to periodic encoding\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# move labels from [0, 1] to [-1, 1] range\n",
    "y_scaled = 2 * (y - 0.5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.28527933e-01  6.31902691e-01  1.13639313e+00  1.44757384e+00]\n",
      " [-7.37687441e-01 -2.07835104e-01 -8.74308565e-01 -1.04211089e+00]\n",
      " [-8.94308978e-01 -1.46744180e+00  3.04378636e-01  3.80566095e-01]\n",
      " [ 4.54202458e-02 -1.67737625e+00  7.89720424e-01  9.14069966e-01]\n",
      " [ 4.54202458e-02  8.41837140e-01 -1.08231219e+00 -1.04211089e+00]\n",
      " [ 1.92487869e+00  2.09934449e-03  1.27506221e+00  1.26973921e+00]\n",
      " [-5.81065904e-01  1.47164049e+00 -6.66304941e-01 -6.86441647e-01]\n",
      " [-4.24444366e-01  8.41837140e-01 -9.43643106e-01 -1.04211089e+00]\n",
      " [ 9.85149470e-01 -2.07835104e-01  1.20572767e+00  1.09190459e+00]\n",
      " [-1.36417359e+00  2.12033793e-01 -1.01297765e+00 -1.04211089e+00]\n",
      " [ 9.85149470e-01 -4.17769553e-01  1.27506221e+00  1.09190459e+00]\n",
      " [-7.37687441e-01  2.12033793e-01 -1.15164673e+00 -1.04211089e+00]\n",
      " [ 4.54202458e-02 -1.25750735e+00  7.89720424e-01  9.14069966e-01]\n",
      " [-1.52079513e+00 -1.67737625e+00 -1.08231219e+00 -8.64276271e-01]\n",
      " [ 8.28527933e-01 -1.88731069e+00  7.89720424e-01  3.80566095e-01]\n",
      " [-5.81065904e-01  4.21968242e-01 -8.04974023e-01 -5.08607024e-01]\n",
      " [-2.67822829e-01  1.26170604e+00 -9.43643106e-01 -1.04211089e+00]\n",
      " [-7.37687441e-01  4.21968242e-01 -1.01297765e+00 -1.04211089e+00]\n",
      " [-5.81065904e-01  1.47164049e+00 -9.43643106e-01 -8.64276271e-01]\n",
      " [-1.11201292e-01  6.31902691e-01 -8.04974023e-01 -1.04211089e+00]\n",
      " [-7.37687441e-01  8.41837140e-01 -1.08231219e+00 -8.64276271e-01]\n",
      " [-1.11201292e-01  1.68157493e+00 -1.08231219e+00 -6.86441647e-01]\n",
      " [-5.81065904e-01  1.47164049e+00 -8.74308565e-01 -1.04211089e+00]\n",
      " [ 1.61163562e+00 -6.27704002e-01  1.20572767e+00  1.26973921e+00]\n",
      " [-7.37687441e-01  6.31902691e-01 -8.74308565e-01 -6.86441647e-01]\n",
      " [-1.05093052e+00 -2.07835104e-01 -1.01297765e+00 -8.64276271e-01]\n",
      " [ 2.02041783e-01 -2.07835104e-01  8.59054966e-01  9.14069966e-01]\n",
      " [ 1.29839254e+00  4.21968242e-01  1.27506221e+00  1.44757384e+00]\n",
      " [-8.94308978e-01  1.05177159e+00 -1.01297765e+00 -1.21994552e+00]\n",
      " [-1.11201292e-01  6.31902691e-01 -9.43643106e-01 -6.86441647e-01]\n",
      " [ 1.45501408e+00  2.12033793e-01  1.13639313e+00  1.26973921e+00]\n",
      " [ 4.54202458e-02  2.31137828e+00 -1.01297765e+00 -1.04211089e+00]\n",
      " [ 3.58663321e-01 -6.27704002e-01  8.59054966e-01  9.14069966e-01]\n",
      " [ 5.15284858e-01 -8.37638451e-01  7.20385883e-01  7.36235342e-01]\n",
      " [ 1.92487869e+00  2.09934449e-03  1.06705859e+00  1.09190459e+00]\n",
      " [ 2.02041783e-01 -1.25750735e+00  7.20385883e-01  5.58400718e-01]\n",
      " [-1.36417359e+00  6.31902691e-01 -1.01297765e+00 -8.64276271e-01]\n",
      " [ 1.14177101e+00 -1.88731069e+00  1.13639313e+00  1.26973921e+00]\n",
      " [ 8.28527933e-01 -4.17769553e-01  1.13639313e+00  1.26973921e+00]\n",
      " [ 3.58663321e-01  2.73124718e+00 -9.43643106e-01 -6.86441647e-01]\n",
      " [-1.36417359e+00  2.09934449e-03 -9.43643106e-01 -1.04211089e+00]\n",
      " [ 9.85149470e-01 -6.27704002e-01  7.89720424e-01  9.14069966e-01]\n",
      " [ 3.58663321e-01 -4.17769553e-01  9.28389507e-01  9.14069966e-01]\n",
      " [ 2.02041783e-01 -8.37638451e-01  9.28389507e-01  9.14069966e-01]\n",
      " [ 2.08150023e+00 -6.27704002e-01  1.34439675e+00  1.09190459e+00]\n",
      " [ 8.28527933e-01 -8.37638451e-01  1.55240038e+00  1.44757384e+00]\n",
      " [ 3.58663321e-01 -6.27704002e-01  1.13639313e+00  9.14069966e-01]\n",
      " [-5.81065904e-01  6.31902691e-01 -9.43643106e-01 -1.04211089e+00]\n",
      " [-7.37687441e-01  6.31902691e-01 -9.43643106e-01 -1.04211089e+00]\n",
      " [ 3.58663321e-01  1.47164049e+00 -8.04974023e-01 -8.64276271e-01]\n",
      " [ 1.45501408e+00 -4.17769553e-01  9.97724048e-01  9.14069966e-01]\n",
      " [-5.81065904e-01  8.41837140e-01 -1.01297765e+00 -8.64276271e-01]\n",
      " [-5.81065904e-01  8.41837140e-01 -1.01297765e+00 -1.04211089e+00]\n",
      " [-4.24444366e-01 -8.37638451e-01  7.20385883e-01  1.09190459e+00]\n",
      " [-1.83403820e+00 -2.07835104e-01 -1.22098127e+00 -1.21994552e+00]\n",
      " [ 2.23812177e+00  2.09934449e-03  1.41373130e+00  1.26973921e+00]\n",
      " [ 4.54202458e-02 -1.46744180e+00  5.81716801e-01  3.80566095e-01]\n",
      " [-5.81065904e-01 -1.25750735e+00  9.63750123e-02  5.58400718e-01]\n",
      " [-1.67741667e+00 -4.17769553e-01 -1.01297765e+00 -1.04211089e+00]\n",
      " [ 4.54202458e-02 -1.46744180e+00  6.51051342e-01  5.58400718e-01]\n",
      " [ 4.54202458e-02 -1.04757290e+00  1.06705859e+00  7.36235342e-01]\n",
      " [ 1.29839254e+00 -1.25750735e+00  1.41373130e+00  1.26973921e+00]\n",
      " [ 1.76825716e+00 -2.07835104e-01  1.06705859e+00  1.09190459e+00]\n",
      " [ 6.71906395e-01 -2.07835104e-01  9.28389507e-01  1.26973921e+00]\n",
      " [-1.11201292e-01 -2.07835104e-01  1.13639313e+00  1.26973921e+00]\n",
      " [ 1.76825716e+00 -4.17769553e-01  1.20572767e+00  9.14069966e-01]\n",
      " [-1.36417359e+00  1.05177159e+00 -1.29031581e+00 -1.04211089e+00]\n",
      " [-1.67741667e+00  2.12033793e-01 -1.08231219e+00 -1.04211089e+00]\n",
      " [ 1.92487869e+00 -2.07835104e-01  1.48306584e+00  1.62540846e+00]\n",
      " [-1.05093052e+00  2.09934449e-03 -8.74308565e-01 -1.04211089e+00]\n",
      " [-7.37687441e-01  8.41837140e-01 -8.74308565e-01 -3.30772400e-01]\n",
      " [ 3.58663321e-01 -2.07835104e-01  9.28389507e-01  7.36235342e-01]\n",
      " [-1.11201292e-01  1.68157493e+00 -8.04974023e-01 -6.86441647e-01]\n",
      " [ 9.85149470e-01 -6.27704002e-01  1.27506221e+00  7.36235342e-01]\n",
      " [-5.81065904e-01  1.26170604e+00 -9.43643106e-01 -6.86441647e-01]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact amplitude encoding may require up to an exponential amount of operations and thus it does not work for NISQ. We can do another type of encoding, i.e. *angle encoding*, for which the features are encoded in as many qubits and each of them is used as a rotation angle of a gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that encodes each of the features as a $R_Z$ rotation for each qubit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_encoding_rz(x):\n",
    "    \"\"\"Encodes features as Z-rotations\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): data point where the entries are the features to be encoded\n",
    "    \"\"\"\n",
    "    # ================\n",
    "    # YOUR CODE BELOW\n",
    "    # ================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quantum kernel methods, the quantum computer is used exclusively to compute the kernel matrix. \n",
    "\n",
    "Remember that the kernel is given by the inner product of two samples $x_1$ and $x_2$. More correctly, it is the inner product between their mappings in the higher-dimensional feature space, thus $\\phi(x_1)$ and $\\phi(x_2)$. For us, this higher-dimensional mapping is given by our $R_Z$-encoding, which leads to the states $|\\phi(x_1)\\rangle$ and $|\\phi(x_2)\\rangle$. Therefore, we want to compute $|\\langle \\phi(x_2) | \\phi(x_1) \\rangle|^2$ (the square matters because the kernel is a measure of a distance).\n",
    "\n",
    "It is easy to verify that the circuit used to compute the kernel is\n",
    "\n",
    "<img src=\"kernel.png\" alt=\"kernel\" width=\"400\"/>\n",
    "\n",
    "where $\\Phi(x)$ computes the feature map and the final measuerement is on the all-zeros projector $|0\\dots 0\\rangle\\langle 0\\dots 0|$ (equivalent to computing the probability of measuring the all-zeros state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_1 = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "projector = np.zeros((2 ** n_qubits, 2 ** n_qubits))\n",
    "projector[0, 0] = 1\n",
    "\n",
    "@qml.qnode(dev_1)\n",
    "def kernel(x1, x2):\n",
    "    \"\"\"Computes the kernel with the R_Z feature map\n",
    "\n",
    "    Args:\n",
    "        x1 (np.ndarray): first sample\n",
    "        x2 (np.ndarray): second sample\n",
    "\n",
    "    Returns:\n",
    "        float: kernel value\n",
    "    \"\"\"\n",
    "    # ================\n",
    "    # YOUR CODE BELOW\n",
    "    # ================\n",
    "    AngleEmbedding(x1, wires=range(n_qubits))\n",
    "    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n",
    "    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel(X_train[0], X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a way to compute the kernel, we can use it to train a support vector machine, which is a classifier. This requires to solve an optimization problem to determine the parameters of the so-called *dual-problem*, which in tuern allow to make predictions for new samples.\n",
    "\n",
    "This procedure is easily taken care of by the scikit-learn library, provided that we build a function for the kernel **matrix**. This has a syntax where the two set of samples can belong to two different datasets A and B, even though, in our case A=B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(A, B):\n",
    "   \"\"\"Returns the kernel matrix from entries in two datasets\n",
    "\n",
    "   Args:\n",
    "       A (List[np.ndarray]): first dataset\n",
    "       B (List[np.ndarray]): second dataset\n",
    "\n",
    "   Returns:\n",
    "       np.ndarray: kernel matrix\n",
    "   \"\"\"\n",
    "   return np.array([[kernel(a, b) for b in B] for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=kernel_matrix).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained SVM can now be used on unseen samples (test data-set) to see how it performs in predicting their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = svm.predict(X_test)\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a variational classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the parity training, here we will be using a variational quantum classifier to classify the Iris dataset. After training, we will check the accuracy score and compare it with the kernel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev_1)\n",
    "def vqc_circuit(theta, x):\n",
    "    \"\"\"Returns the variational quantum circuit with angle embedding and strongly entangling layers\n",
    "    as ansatz\n",
    "\n",
    "    Args:\n",
    "        theta (tensor_like): trainable ansatz parameters. The first dimension determines the number of\n",
    "            layers. Shape = (L, M ,3); L = layers, M = n_qubits, 3 = rotations per qubit.\n",
    "        x (np.ndarray): sample (properly scaled)\n",
    "\n",
    "    Returns:\n",
    "        float: prediction of the quantum model\n",
    "    \"\"\"\n",
    "\n",
    "    # embedding\n",
    "    AngleEmbedding(x, wires=range(n_qubits))\n",
    "\n",
    "    # trainable measurement\n",
    "    StronglyEntanglingLayers(theta, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqc_with_bias(theta, bias, x):\n",
    "    \"\"\"Adds bias to the outcome of a vqc\n",
    "\n",
    "    Args:\n",
    "        theta (tensor_like): trainable ansatz parameters\n",
    "        bias (float): classical bias\n",
    "        x (np.ndarray): sample (properly scaled)\n",
    "\n",
    "    Returns:\n",
    "        float: (vqc output) + bias\n",
    "    \"\"\"\n",
    "    return vqc_circuit(theta, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    \"\"\"Computes the MSE between labels and predictions\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): actual values\n",
    "        predictions (List[int]): model predictions\n",
    "\n",
    "    Returns:\n",
    "        float: value of the MSE\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l - p) ** 2\n",
    "\n",
    "    loss /= len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, bias, X, y):\n",
    "    \"\"\"Computes the predictions and returns the MSE over the dataset\n",
    "\n",
    "    Args:\n",
    "        theta (List[np.ndarray]): all the variational parameters, one\n",
    "            array per layer\n",
    "        bias (float): classical bias value, added to the output of the quantum circuit\n",
    "        X (List[List]): list of all train samples\n",
    "        y (List): labels\n",
    "\n",
    "    Returns:\n",
    "        float: MSE over the dataset\n",
    "    \"\"\"\n",
    "    predictions = [vqc_with_bias(theta, bias, x) for x in X]\n",
    "    return square_loss(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.0015854   0.0062559  -0.00309347]\n",
      "  [ 0.00456775 -0.00661926 -0.00363054]\n",
      "  [-0.00381738 -0.0119584   0.00486972]\n",
      "  [-0.00469402  0.00012494  0.00480747]]\n",
      "\n",
      " [[ 0.00446531  0.00665385 -0.00098485]\n",
      "  [-0.00423298 -0.00079718 -0.01687334]\n",
      "  [-0.01447112 -0.013227   -0.00997247]\n",
      "  [ 0.00399774 -0.00905479 -0.00378163]]\n",
      "\n",
      " [[ 0.01299228 -0.00356264  0.00737516]\n",
      "  [-0.00933618 -0.00205438 -0.00950022]\n",
      "  [-0.00339033  0.00840308 -0.0172732 ]\n",
      "  [ 0.00434424  0.00237736 -0.0059415 ]]\n",
      "\n",
      " [[-0.01446058  0.0007213  -0.00529493]\n",
      "  [ 0.00232676  0.00021852  0.01601779]\n",
      "  [-0.00239356 -0.01023497  0.00179276]\n",
      "  [ 0.00219997  0.01359188  0.00835111]]\n",
      "\n",
      " [[ 0.00356871  0.01463303 -0.01188763]\n",
      "  [-0.00639752 -0.00926576 -0.0038981 ]\n",
      "  [-0.01376686  0.00635151 -0.00222223]\n",
      "  [-0.01470806 -0.01015579  0.00313514]]\n",
      "\n",
      " [[ 0.00838127  0.01996731  0.02913862]\n",
      "  [ 0.00414409 -0.00989538 -0.02132046]\n",
      "  [ 0.00267711 -0.00812941 -0.00415357]\n",
      "  [-0.00612097 -0.00140791  0.0106598 ]]]\n"
     ]
    }
   ],
   "source": [
    "n_layers = 6\n",
    "n_iters = 100\n",
    "\n",
    "theta_init = .01 * rng.normal(size=(n_layers, n_qubits, 3), requires_grad=True)\n",
    "print(theta_init)\n",
    "bias_init = np.array(0., requires_grad=True)\n",
    "# print(bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.0769043 | Acc train: 0.4400000 | Acc validation: 0.3600000 \n",
      "Iter:     2 | Cost: 1.0748452 | Acc train: 0.4533333 | Acc validation: 0.3600000 \n",
      "Iter:     3 | Cost: 1.0722913 | Acc train: 0.4533333 | Acc validation: 0.3600000 \n",
      "Iter:     4 | Cost: 1.0688859 | Acc train: 0.4666667 | Acc validation: 0.3600000 \n",
      "Iter:     5 | Cost: 1.0630089 | Acc train: 0.4800000 | Acc validation: 0.3600000 \n",
      "Iter:     6 | Cost: 1.0588640 | Acc train: 0.4666667 | Acc validation: 0.4000000 \n",
      "Iter:     7 | Cost: 1.0554286 | Acc train: 0.4800000 | Acc validation: 0.4000000 \n",
      "Iter:     8 | Cost: 1.0547370 | Acc train: 0.4800000 | Acc validation: 0.4400000 \n",
      "Iter:     9 | Cost: 1.0517812 | Acc train: 0.4800000 | Acc validation: 0.4400000 \n",
      "Iter:    10 | Cost: 1.0496193 | Acc train: 0.4800000 | Acc validation: 0.4400000 \n",
      "Iter:    11 | Cost: 1.0513859 | Acc train: 0.4800000 | Acc validation: 0.5600000 \n",
      "Iter:    12 | Cost: 1.0500976 | Acc train: 0.4933333 | Acc validation: 0.6000000 \n",
      "Iter:    13 | Cost: 1.0516298 | Acc train: 0.4800000 | Acc validation: 0.6400000 \n",
      "Iter:    14 | Cost: 1.0547721 | Acc train: 0.4800000 | Acc validation: 0.6000000 \n",
      "Iter:    15 | Cost: 1.0562622 | Acc train: 0.4800000 | Acc validation: 0.6000000 \n",
      "Iter:    16 | Cost: 1.0507858 | Acc train: 0.4800000 | Acc validation: 0.6000000 \n",
      "Iter:    17 | Cost: 1.0434666 | Acc train: 0.5066667 | Acc validation: 0.6400000 \n",
      "Iter:    18 | Cost: 1.0338059 | Acc train: 0.5200000 | Acc validation: 0.6000000 \n",
      "Iter:    19 | Cost: 1.0220073 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    20 | Cost: 1.0107766 | Acc train: 0.5200000 | Acc validation: 0.5600000 \n",
      "Iter:    21 | Cost: 0.9961072 | Acc train: 0.5066667 | Acc validation: 0.5600000 \n",
      "Iter:    22 | Cost: 0.9771922 | Acc train: 0.5600000 | Acc validation: 0.4400000 \n",
      "Iter:    23 | Cost: 0.9610140 | Acc train: 0.5466667 | Acc validation: 0.4400000 \n",
      "Iter:    24 | Cost: 0.9445848 | Acc train: 0.5733333 | Acc validation: 0.4400000 \n",
      "Iter:    25 | Cost: 0.9303336 | Acc train: 0.6000000 | Acc validation: 0.5200000 \n",
      "Iter:    26 | Cost: 0.9116498 | Acc train: 0.6133333 | Acc validation: 0.5200000 \n",
      "Iter:    27 | Cost: 0.8877719 | Acc train: 0.6133333 | Acc validation: 0.5200000 \n",
      "Iter:    28 | Cost: 0.8611685 | Acc train: 0.6400000 | Acc validation: 0.5200000 \n",
      "Iter:    29 | Cost: 0.8288462 | Acc train: 0.6666667 | Acc validation: 0.5200000 \n",
      "Iter:    30 | Cost: 0.7957690 | Acc train: 0.6933333 | Acc validation: 0.6000000 \n",
      "Iter:    31 | Cost: 0.7585224 | Acc train: 0.7600000 | Acc validation: 0.6800000 \n",
      "Iter:    32 | Cost: 0.7214285 | Acc train: 0.7866667 | Acc validation: 0.7200000 \n",
      "Iter:    33 | Cost: 0.6849394 | Acc train: 0.7866667 | Acc validation: 0.8400000 \n",
      "Iter:    34 | Cost: 0.6496503 | Acc train: 0.8133333 | Acc validation: 0.8800000 \n",
      "Iter:    35 | Cost: 0.6174372 | Acc train: 0.8133333 | Acc validation: 0.9200000 \n",
      "Iter:    36 | Cost: 0.5877130 | Acc train: 0.8266667 | Acc validation: 0.9200000 \n",
      "Iter:    37 | Cost: 0.5602478 | Acc train: 0.8533333 | Acc validation: 0.9200000 \n",
      "Iter:    38 | Cost: 0.5339506 | Acc train: 0.8533333 | Acc validation: 0.9200000 \n",
      "Iter:    39 | Cost: 0.5093042 | Acc train: 0.8666667 | Acc validation: 0.9200000 \n",
      "Iter:    40 | Cost: 0.4859169 | Acc train: 0.8666667 | Acc validation: 0.9200000 \n",
      "Iter:    41 | Cost: 0.4638077 | Acc train: 0.8800000 | Acc validation: 0.9200000 \n",
      "Iter:    42 | Cost: 0.4439465 | Acc train: 0.8933333 | Acc validation: 0.9200000 \n",
      "Iter:    43 | Cost: 0.4250991 | Acc train: 0.8933333 | Acc validation: 0.9200000 \n",
      "Iter:    44 | Cost: 0.4075764 | Acc train: 0.9066667 | Acc validation: 0.9200000 \n",
      "Iter:    45 | Cost: 0.3916878 | Acc train: 0.9066667 | Acc validation: 0.9200000 \n",
      "Iter:    46 | Cost: 0.3772355 | Acc train: 0.9066667 | Acc validation: 0.9200000 \n",
      "Iter:    47 | Cost: 0.3644449 | Acc train: 0.9333333 | Acc validation: 0.9200000 \n",
      "Iter:    48 | Cost: 0.3524483 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    49 | Cost: 0.3416121 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    50 | Cost: 0.3315397 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    51 | Cost: 0.3223000 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    52 | Cost: 0.3129754 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    53 | Cost: 0.3035386 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    54 | Cost: 0.2928739 | Acc train: 0.9466667 | Acc validation: 0.9200000 \n",
      "Iter:    55 | Cost: 0.2830654 | Acc train: 0.9600000 | Acc validation: 0.9200000 \n",
      "Iter:    56 | Cost: 0.2732587 | Acc train: 0.9733333 | Acc validation: 0.9200000 \n",
      "Iter:    57 | Cost: 0.2646024 | Acc train: 0.9733333 | Acc validation: 0.9200000 \n",
      "Iter:    58 | Cost: 0.2560226 | Acc train: 0.9733333 | Acc validation: 0.9600000 \n",
      "Iter:    59 | Cost: 0.2481986 | Acc train: 0.9866667 | Acc validation: 0.9600000 \n",
      "Iter:    60 | Cost: 0.2413635 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    61 | Cost: 0.2351523 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    62 | Cost: 0.2288153 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    63 | Cost: 0.2221886 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    64 | Cost: 0.2160229 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    65 | Cost: 0.2103508 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    66 | Cost: 0.2049596 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    67 | Cost: 0.1999048 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    68 | Cost: 0.1952180 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    69 | Cost: 0.1909417 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    70 | Cost: 0.1870688 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    71 | Cost: 0.1834513 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    72 | Cost: 0.1800433 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    73 | Cost: 0.1768394 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    74 | Cost: 0.1739182 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    75 | Cost: 0.1711019 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    76 | Cost: 0.1682611 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    77 | Cost: 0.1654655 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    78 | Cost: 0.1630051 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    79 | Cost: 0.1607516 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    80 | Cost: 0.1583964 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    81 | Cost: 0.1561268 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    82 | Cost: 0.1541483 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    83 | Cost: 0.1522621 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    84 | Cost: 0.1505347 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    85 | Cost: 0.1489422 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    86 | Cost: 0.1472217 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    87 | Cost: 0.1455437 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    88 | Cost: 0.1442162 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    89 | Cost: 0.1429422 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:    90 | Cost: 0.1417975 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    91 | Cost: 0.1406544 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    92 | Cost: 0.1397683 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    93 | Cost: 0.1389311 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    94 | Cost: 0.1380423 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    95 | Cost: 0.1372134 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    96 | Cost: 0.1359859 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    97 | Cost: 0.1344144 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    98 | Cost: 0.1328878 | Acc train: 1.0000000 | Acc validation: 1.0000000 \n",
      "Iter:    99 | Cost: 0.1315936 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n",
      "Iter:   100 | Cost: 0.1305895 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n"
     ]
    }
   ],
   "source": [
    "opt = NesterovMomentumOptimizer(0.01)\n",
    "batch_size = 10\n",
    "\n",
    "# train the variational classifier\n",
    "theta = theta_init\n",
    "bias = bias_init\n",
    "for it in range(n_iters):\n",
    "\n",
    "    # Update the weights for each optimizer step\n",
    "    batch_index = rng.integers(0, len(X_train), size=batch_size)\n",
    "    X_batch = np.array(X_train[batch_index])\n",
    "    y_batch = np.array(y_train[batch_index])\n",
    "    theta, bias, _, _ = opt.step(cost, theta, bias, X_batch, y_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(vqc_with_bias(theta, bias, x)) for x in X_train]\n",
    "    predictions_test = [np.sign(vqc_with_bias(theta, bias, x)) for x in X_test]\n",
    "\n",
    "    # Compute accuracy on train and test set\n",
    "    acc_train = accuracy_score(y_train, predictions_train)\n",
    "    acc_test = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(theta, bias, X_train, y_train), acc_train, acc_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ba57309a75c1f2334b12eaf868931394174054c28fafce8b57219f88302837c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('plTutorialsEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
