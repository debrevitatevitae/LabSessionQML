{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook has two purposes, tackled in two examples. In both, a variational circuit is trained. In the first one, the aim is to learn the parity function, which requires that a binary input is encoded. In the second example, the Iris dataset is learned, which requires real amplitude vectors to be encoded as amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the parity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    \"\"\"Applies a layer of arbitrary rotations and circular entanglements to the variational circuit\n",
    "\n",
    "    Args:\n",
    "        W (np.ndarray): rotation parameters for the layer\n",
    "    \"\"\"\n",
    "    for i in range(n_qubits):\n",
    "        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "\n",
    "    for i in range(n_qubits-1):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "\n",
    "    if n_qubits > 2:\n",
    "        qml.CNOT(wires=[n_qubits-1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x):\n",
    "    \"\"\"Prepares the binary state fed to the vqc\n",
    "\n",
    "    Args:\n",
    "        x (List): list of 0s and 1s corresponding to the basis state\n",
    "    \"\"\"\n",
    "    qml.BasisState(x, wires=[i for i in range(n_qubits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `qnode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    \"\"\"Generates the trainable quantum circuit with binary input\n",
    "\n",
    "    Args:\n",
    "        weights (List[np.ndarray]): all the variational parameters, one\n",
    "            array per layer\n",
    "        x (List): binaries representing the input\n",
    "    \"\"\"\n",
    "    statepreparation(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a classical bias parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    \"\"\"Complete variational classifier, including a bias\n",
    "\n",
    "    Args:\n",
    "        weights (List[np.ndarray]): all the variational parameters, one\n",
    "            array per layer\n",
    "        bias (float): classical bias value, added to the output of the quantum circuit\n",
    "        x (List): binaries representing the input\n",
    "    \"\"\"\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    \"\"\"Computes the MSE between labels and predictions\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): actual values\n",
    "        predictions (List[int]): model predictions\n",
    "\n",
    "    Returns:\n",
    "        float: value of the MSE\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l - p) ** 2\n",
    "\n",
    "    loss /= len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    \"\"\"Returns the accuracy over the dataset\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): true values\n",
    "        predictions (List[int]): model-predicted values\n",
    "\n",
    "    Returns:\n",
    "        float: number of accurate predictions over total\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss += 1\n",
    "\n",
    "    loss /= len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    \"\"\"Computes the predictions and returns the MSE over the dataset\n",
    "\n",
    "    Args:\n",
    "        weights (List[np.ndarray]): all the variational parameters, one\n",
    "            array per layer\n",
    "        bias (float): classical bias value, added to the output of the quantum circuit\n",
    "        X (List[List]): list of all the binary input strings\n",
    "        Y (List): labels\n",
    "\n",
    "    Returns:\n",
    "        float: MSE over the dataset\n",
    "    \"\"\"\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/parity.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data[:, :-1], requires_grad=False)\n",
    "Y = np.array(data[:, -1], requires_grad=False)\n",
    "\n",
    "# shift lables from [0, 1] to [-1, 1], to match the range of expectation values\n",
    "Y = 2 * Y - np.ones(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.0012573  -0.00132105  0.00640423]\n",
      "  [ 0.001049   -0.00535669  0.00361595]\n",
      "  [ 0.01304     0.00947081 -0.00703735]\n",
      "  [-0.01265421 -0.00623274  0.00041326]]\n",
      "\n",
      " [[-0.02325031 -0.00218792 -0.01245911]\n",
      "  [-0.00732267 -0.00544259 -0.003163  ]\n",
      "  [ 0.00411631  0.01042513 -0.00128535]\n",
      "  [ 0.01366463 -0.00665195  0.0035151 ]]] 0.0\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "num_layers = 2\n",
    "weights_init = .01 * rng.normal(size=(num_layers, n_qubits, 3), requires_grad=True)\n",
    "bias_init = np.array(0., requires_grad=True)\n",
    "\n",
    "print(weights_init, bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimizer and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 2.1594871 | Accuracy: 0.5000000 \n",
      "Iter:     2 | Cost: 2.1571387 | Accuracy: 0.5000000 \n",
      "Iter:     3 | Cost: 3.3674659 | Accuracy: 0.5000000 \n",
      "Iter:     4 | Cost: 1.3990385 | Accuracy: 0.5000000 \n",
      "Iter:     5 | Cost: 1.0404263 | Accuracy: 0.5000000 \n",
      "Iter:     6 | Cost: 0.9132724 | Accuracy: 0.5000000 \n",
      "Iter:     7 | Cost: 0.1498629 | Accuracy: 1.0000000 \n",
      "Iter:     8 | Cost: 0.0293096 | Accuracy: 1.0000000 \n",
      "Iter:     9 | Cost: 0.2336926 | Accuracy: 1.0000000 \n",
      "Iter:    10 | Cost: 0.3322166 | Accuracy: 1.0000000 \n",
      "Iter:    11 | Cost: 0.2453625 | Accuracy: 1.0000000 \n",
      "Iter:    12 | Cost: 0.0481789 | Accuracy: 1.0000000 \n",
      "Iter:    13 | Cost: 0.0038142 | Accuracy: 1.0000000 \n",
      "Iter:    14 | Cost: 0.0038158 | Accuracy: 1.0000000 \n",
      "Iter:    15 | Cost: 0.0334435 | Accuracy: 1.0000000 \n",
      "Iter:    16 | Cost: 0.0920158 | Accuracy: 1.0000000 \n",
      "Iter:    17 | Cost: 0.0215532 | Accuracy: 1.0000000 \n",
      "Iter:    18 | Cost: 0.0138518 | Accuracy: 1.0000000 \n",
      "Iter:    19 | Cost: 0.0060476 | Accuracy: 1.0000000 \n",
      "Iter:    20 | Cost: 0.0048316 | Accuracy: 1.0000000 \n",
      "Iter:    21 | Cost: 0.0009670 | Accuracy: 1.0000000 \n",
      "Iter:    22 | Cost: 0.0004262 | Accuracy: 1.0000000 \n",
      "Iter:    23 | Cost: 0.0003499 | Accuracy: 1.0000000 \n",
      "Iter:    24 | Cost: 0.0005918 | Accuracy: 1.0000000 \n",
      "Iter:    25 | Cost: 0.0005103 | Accuracy: 1.0000000 \n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "iterations = 25\n",
    "\n",
    "for it in range(iterations):\n",
    "\n",
    "    # shuffle the batch indices\n",
    "    batch_index = rng.integers(0, len(X), size=batch_size)\n",
    "\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "\n",
    "    # update weights and biases\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "    # compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X, Y), acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ba57309a75c1f2334b12eaf868931394174054c28fafce8b57219f88302837c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('plTutorialsEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
